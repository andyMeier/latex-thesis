\section{Conclusion}
This paper presents empirical evidence for the influence of accuracy and explainability on user trust. First, we generated truthful minimal explanations and nonsensical explanations for three systems with different performance levels. We then validated the explanations' fidelity by reducing sample points to input features used for the explanations, and re-classifying those sample points a second time with the same classifiers. We built a use scenario and tested the explanations in a user study. The users' trust was measured with a self-reporting questionnaire and a proxy measure based on observations of how the participants' classifications were influenced by seeing the explanations.\newline
Our findings show that users have the most trust in systems without explanations, i.e. minimum explanations can potentially harm, but not improve user trust. We argue that the act of reconciling conflicting information of the mental model and the given explanations counts as a deceptive experience and therefore affects the user's trust negatively. If an explanation is added to a system (e.g. for increasing user's understanding of the system), its truthfulness is crucial for user trust. We saw that for systems with a medium accuracy (0.76), a truthful explanation does not harm user trust, while a nonsensical explanation decreases trust. Overall, the systems' accuracy levels were most decisive for user trust: the higher the accuracy, the higher the user's trust.\newline
Further research with more rich explanations and a detailed investigation of trust factors is needed to examine potential positive effects of explanations on user trust. The development of trust over time should also be researched in the future, to give valuable directions to xAI practitioners implementing explanations in productive systems.