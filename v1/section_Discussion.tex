\section{Discussion}
%-------------------------------------------------------------------------
% RQ 1: effect of accuracy on trust
\paragraph{Accuracy} 
Our results suggest that \textit{both perceived understanding and trust are positively related to the classifier's accuracy in general} (\textbf{RQ 1}). The higher the accuracy, the higher the trust in the system. The strongest evidence is found in the reactions to the classifiers without explanations (\textit{super-no}, \textit{medium-no}, \textit{bad-no}). The self-reported trust was the highest for \textit{super-no} and the lowest for \textit{bad-no}, with all scores differing significantly from each other. The same can be observed even when adding nonsensical explanations: The \textit{super-rand} still has significantly higher trust and perceived understanding scores than \textit{medium-rand} and \textit{bad-rand}, with the \textit{bad-rand} again having the lowest scores. The good explanation, however, influences the trust and perceived understanding differently. Here, \textit{bad-good} still receives significantly worse trust and understanding scores than both \textit{medium-good} and \textit{super-good}, yet there is no difference anymore in the scores of \textit{medium-good} and \textit{super-good}. An explanation for the similarity of both the trust score and the perceived understanding score for \textit{super-good} and \textit{medium-good} could be the persuasiveness of a good explanation. The difference in trust and perceived understanding that we see between \textit{super-no} and \textit{medium-no} could be compensated by convincing the user of the classifier's trustworthiness through a good explanation.\newline
It seems intuitive to have higher trust in a system that leads to fewer deception, which has also been described in \cite{glass2008toward} with the ``expectation mismatch" (see section \ref{subsubsec:trust_factors}). A classifier with high accuracy effectively leads to fewer disappointed expectations, which in turn does not decrease the trust. Furthermore, the set size of 15 Tweets seems to be enough for users to develop an intuition about the classifier's accuracy.\newline
%-----------------------------------------------------------------------
% RQ 2.1: presence of explanation
\paragraph{Presence of Explanations} 
In the copy machine experiment by \cite{langer1978mindlessness}, only the pure presence of an explanation was enough to make people comply with a request resulting in a short waiting time. On the basis of that experiment, we designed three explanations similar to the setup in \cite{langer1978mindlessness}: No explanation, placebic explanation, and a meaningful explanation for the classifier's behaviour. Similar to the results of the copy machine experiment, we expected to see no difference between trust scores of the meaningful and placebic explanation, but a difference between the two explanations and the no explanation settings. The results, however, lead to a different conclusion: In our experiment, \textit{the presence of an explanation did not have a positive effect on trust in any of the conditions} (\textbf{RQ 2.1}). Adding an explanation to the system influenced the trust in the very good and the medium classifier; it decreased the trust in the very good classifier and did not matter for the bad classifier. For the medium classifier, the type of explanation was crucial for its influence - a good explanation raised trust to the level of no explanation, while a nonsensical decreased trust levels.\newline
For the \textit{very good classifier}, other than expected, \textit{super-no} shows better results than \textit{super-good} and \textit{super-rand}. The classifier performed at an accuracy of 0.95, which resulted in 44\% of the cases in a perfect classification rate within the small subset of 15 Tweets. A possible explanation for the good trust score of \textit{super-no} could be the conservation of a perfect image throughout the 15 Tweets. The classifier makes (almost) no mistakes and does not offer any information that could lead to doubts about the classifier's abilities. Both \textit{super-good} and \textit{super-rand} would then have a disadvantage over the no explanation condition: The good explanations are not necessarily meaningful to a human, as they are based on statistical information rather than semantics or intentions. The placebic explanation is generated at random, which likewise holds potential for doubts and incomprehension. A similar guess was ventured in \cite{cramer2008effects}, who suspected that more knowledge about system boundaries and unfulfilled preferences leads to a decrease in trust (see section \ref{subsubsec:trust_factors}). The opposite can be observed in the proxy measure for trust, i.e. the changes in labelling that a user made towards the classifier's decision but away from the truth. Here, \textit{super-no} led to significantly fewer changes away from the truth as compared to \textit{super-good} and \textit{super-rand}, which is in turn consistent with the results in the copy machine experiment. It is important to note that the copy machine experiment only worked while people are in a ``mindless" state, i.e. an inattentive state of mind. It is possible that users did not in particular pay attention to their trust towards the system during the classification task, but actively reflected on their relationship with the system during the self-report of trust. Being in a mindless state during the proxy measurement while being mindful during the trust questionnaire would explain the conflicting results of both measures.\newline
The result for the \textit{medium classifier} differs from that of the very good classifier. The trust score of \textit{medium-no} does not differ significantly from \textit{medium-good}, but is higher than the score of \textit{medium-rand}. While the very good classifier had lower trust scores for \textit{super-good} than for \textit{super-no}, the medium classifier shows an equal trust score for good and no explanations. The medium classifier has a lower accuracy and makes three to four mistakes on each subset. It is imaginable that users are more conscious about the classifier's behaviour than they were with the (almost) perfect system due to the higher error rate. Seeing a meaningful explanation about why the classifier came to a faulty decision could raise the trust, while not seeing the reasons for a classification mistake could decrease trust - eventually ending up at the same level.\newline
The \textit{bad classifier} did not show evidence of diverting trust scores for any of the three explanation types. The same homogeneity is found in the results of the proxy measurement of trust, for both the changes away from the truth and towards the truth. The trust scores were significantly lower than any other condition, with one exemption: \textit{bad-no} had comparable trust ratings as \textit{medium-rand}. Since there is a significant distance between the scores of \textit{medium-rand} and both \textit{medium-good} and \textit{medium-no}, we conclude that it is due to a property of the medium-random system rather than a phenomenon of the bad classifier.\newline
We note that no condition with explanation leads to a significantly higher trust level than no explanation. Our findings therefore do not support the claims made in literature about a positive effect of explanations on user trust (see section \ref{subsubsec:Explanation Goals}). More drastically, if both explanation types fail to receive higher scores than no explanation, users seem to have ``blind faith" that can be harmed (but not increased) by being informed about the system. Yet, offering an explanation does not inevitably harm the user's trust. For the medium and bad classifier, the good explanation led to an equal amount of trust as no explanation. The users of the systems with good explanations, however, have knowledge about the system and potentially a more truthful mental model. If that is the case (which should be confirmed in future research), those users are more able to judge about the system in terms of fairness, flaws, or completeness.\newline
%--------------------------------------------------------------------------------
% RQ 2.2: level of truthfulness
\paragraph{Level of Truthfulness} 
Our findings furthermore show that \textit{the level of truthfulness of an explanation does not make a difference for a very good nor for a very bad classifier, set does play a role for the medium classifier} \textbf{RQ 2.2}. Only for a moderate classifier with an accuracy of 0.76 (a quite realistic score for real-life applications), the explanation fidelity is important: more truthfulness leads to more trust here.\newline
For the \textit{very good classifier}, \textit{super-good} and \textit{super-rand} led to the same trust score, while \textit{super-no} had a significantly higher trust score. As explained before, both explanation types (\textit{super-good} and \textit{super-rand}) give the opportunity to notice inconsistencies, weak explanations or contradicting evidence. In this case, the level of truthfulness does not seem to make a difference, as any explanation decreases the trust. In our experiment, we chose the ``minimum explanation" approach, only showing the relation between input features and classification result. It is possible that the resolution is not high enough to see differences in trust for \textit{super-good} and \textit{super-rand}. A more detailed explanation, explaining for example the model structure, or similar and dissimilar cases, could enhance subtle differences between the two conditions, if any.\newline
\textit{medium-good} and \textit{medium-rand}, however, have significantly different trust scores: users trust \textit{medium-good} more than \textit{medium-rand}. The medium classifier delivers faulty classifications in three to four cases out of 15, which presumably raises doubts about the system. The negative effect of placebic explanations could therefore be worsened. The ``expectation mismatch" is then twofold, with the wrong classification on the one side and the useless explanation on the other side. With the good explanations, the users receive some information about the underlying reasons for a misclassification. Even if not all information of the explanation is meaningful to a human, it delivers hints to the system's function and malfunction, possibly raising overall trust.\newline
The evidence suggests that users are not fooled by a \textit{bad classifier} and do not trust it, no matter the explanation given. There is no significant difference between \textit{bad-good} and \textit{bad-rand}. Although a plausible assumption could have been that users trust a bad classifier because it is predictable, yet do not use it as a basis for their decisions because it is not accurate. A possible topic of future work could be a distinct measure of predictability as a characteristic of trust, to examine this assumption in more detail.\newline
%--------------------------------------------------------------------------------
% RQ 3: effect of explanation types on perceived understanding
\paragraph{Perceived Understanding} 
One of the factors contributing to trust is \textit{perceived understanding}. Our findings show a negative influence of meaningless information on perceived understanding (\textbf{RQ 3}). For both the medium classifier and the very good classifier, perceived understanding was the worst when delivering placebic explanations. Although actual understanding is arguably different when comparing a case without any explanation and one with good, meaningful information, the perception of knowledge about both cases is equal here. A mechanism similar to ``expectation mismatch" could be in place for perception of knowledge. While building the mental model of the classifiers, no conflicting information have to be consolidated for the good explanation and the no explanation cases. Being confronted with random and therefore meaningless explanations forces the user to unite conflicting information in the mental model. The more conflicts appear, the lower the confidence in the mental model.\newline
For the bad classifier, perceived understanding ratings are significantly lower as for other classifiers (except for medium-random, which has a low rating as well). However, the system delivering good explanations for the faulty behaviour receives a significantly higher score than no explanation and placebic explanation cases. The positive effect of high accuracy does not hold here because the classifier performs badly on the task. Yet, as the explanations give more information about the inner workings of the classifier, it seems intuitive to evoke more confidence of understanding in this case.\newline
%--------------------------------------------------------------------------------
% Further discussions
\paragraph{Trust proxy}
The proxy measurement of trust via changes in classification between the first and second block of Tweets showed ambiguous results with high variance. For research dealing with trust in computer systems, it could be useful to measure trust not only via a questionnaire that requires reflection abilities and active processing of the relationship between user and system. In our results, we see that the proxy measure reporting the observed (hence potentially unconscious) trust does not always agree with the self-reported trust. While users of \textit{super-no} report the highest trust score, they are not as easy ``lured" to make a wrong classification as the users of \textit{super-good}. If this is due to an actual gap between actions and reflections of users, the proxy measure could be more interesting for xAI practitioners as it shows how users actually interact with a system. Using a trust measure that can be determined without the participant knowing could also serve as an additional view on user trust.\medskip \newline

%--------------------------------------------------------------------------------
% Further discussions
{\color{blue} die nÃ¤chsten zwei Abschnitte noch nicht bitte}
\paragraph{Limitations \& future work}
An aspect of human-machine trust that could not be covered in this study is the evolution of trust over time. We used willingness to accept a computer-generated classification result as a proxy measure for trust and observed the changes in manual classification away from the truth in favour of the classifier's decision. For \textit{super-good}, we observed a higher value of that proxy than in any other classifier-explanation condition. The validity of those results are, however, influenced by the low number of data points: only 14 participants in \textit{super-good} had the opportunity to show such a behaviour. Other participants in the same condition had a subset with perfect classification, hence no opportunity for the classifier to misclassify and subsequently convince the user that the classification is correct nevertheless. Of those 14 participants, 4 showed the behaviour in question. This number is too small to analyse in what stage of the relationship the started to trust the classifier. For further research, it would therefore be interesting to investigate how trust develops over time: Whether users start with a high trust level and decrease the trust with every mismatch, or have a basic trust level that is increased with expectation matches and decreased with every mismatch remains to be examined in future research.\newline
Our results also do not deliver information about the proportionality of accuracy and trust level. We consider two ``extreme" cases (a very good and a very bad classifier), as well as one medium classifier with a supposedly more realistic accuracy level (0.76). In future research, more classifiers in the range between the two extremes should be investigated. We saw that for the medium classifier, the explanation type matters, but it did not for the bad classifier. From our study, we cannot determine at what accuracy level this is the case. A similar phenomenon is happening towards the upper extreme: a good explanation decreases trust for high-accuracy systems, but not for medium-accuracy systems. Determining the accuracy level at which a good explanation starts to harm user trust could be useful for practitioners.\newline
For generating explanations, we used the minimum explanation approach. The approach is limited in that it does not deliver information about the inner structure of a classifier, but only about the relation between input and output. As discussed before, examining the influence of the explanation's ``explanatory power" could give valuable information for xAI practitioners. \cite{ribeiro2018anchors} compared an explanation with little explanatory power (one if-then rule) to an explanation with more explanatory power (two if-then rules) and did not find evidence that more explanatory power leads to better understanding of the systems. Although they increased the explanation content, they only varied the amount of if-then rules instead of giving information of different kind (the classification probability, similar or dissimilar cases, the model structure, etc.).\newline

%--------------------------------------------------------------------------------
% Further discussions
\paragraph{Societal impact}
a warning sign that we need to avoid inappropriate trust in computer systems, but:
accuracy weighs strongest --> people not easy to trick
high-level study, should be tested again with actual discrimination (not easy to get the data, though)

