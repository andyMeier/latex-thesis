\section{Introduction}
\label{sec:intro}
Deploying machine learning algorithms in applications to support human decision-making is no exception anymore. Automated systems using non-transparent algorithms are no longer restricted solely to computationally heavy applications such as information retrieval or computer graphics \cite{liu2017towards}, but can be found in human-centred areas as well. Medical diagnosis, insurance risk analysis, and self-driving cars are examples of areas with a high potential for utilising machine learning systems \cite{guidotti2018survey}. Other areas have already replaced human decision-making with machine learning: Recommender systems for films and music, decision systems for targeted advertisements, or credit rating assessments take decisions without human intervention in the application \cite{gilpin2018explaining}. Machine learning systems can also be used as a source of knowledge and additional information to support a human in the decision-making process. The collaboration of machine learning systems and humans with the goal to extend the cognitive abilities of humans is called \textit{augmented intelligence} \cite{ventocilla2018taxonomy}.\newline
In a collaboration setting, a human collaborator or an end user needs to judge how reliable and trustworthy the output is. Understanding what brought about the decisions therefore becomes a challenge for machine learning systems deployed in the real world. \textit{Interpretability} describes how well a machine learning classifier can be understood \cite{kotsiantis2007supervised}. One advantage of interpretability is the early detection and avoidance of faulty behaviour. Unexpected algorithmic behaviour can for example originate from biases in the data set, systematic errors in the classifier's design, or intentional alteration for criminal purposes \cite{gilpin2018explaining}. Especially for high-risk domains such as terrorism detection or mining of health data, detecting anomalies is crucial \cite{ribeiro2016should} and ideally happens before deploying and relying on the system. Another reason for avoiding opaque decision systems is fairness, inclusion, and control over personal data \cite{diakopoulos2016accountability, gilpin2018explaining, goodman16eu}. End users should have the possibility to investigate whether they have been judged adequately by an automated system \cite{selbst2017meaningful}. Likewise, engineers who want to prevent \textit{automated discrimination} profit from transparency \cite{ribeiro2016should, richardson2018survey}. In general, interpretability is not only a beneficial, but a critical characteristic for applications with a potential for serious consequences of faulty decisions \cite{richardson2018survey}. Additionally, explainability fosters \textit{trust} in the system  \cite{biran2017explanation, cramer2008effects, diakopoulos2016accountability, preece2018asking, vorm2018assessing} which ``make[s] a user (the audience) feel comfortable with a prediction or decision so that they keep using the system" \cite{van2001perceived}.\newline
Explanations for machine learning classifiers have been discussed in literature. 
%Explaining the decisions of an automated computer system can focus on different aspects of the system depending on the task at hand \cite{biran2017explanation}. A \textit{global explanation} aims to provide macro information on the structure of the model and its behaviour in general, while a \textit{local explanation} deals with a single decision and the factors that produced this particular decision. A third explanation focus targets the \textit{input features}. But not all systems can be exhaustively explained. 
Whereas some models are inherently interpretable (e.g. decision trees, Naive Bayes, rule-based systems to some extent of complexity \cite{kotsiantis2007supervised}), others are inherently non-interpretable (e.g. artificial neural networks, deep learning algorithms). Additionally, the trend of machine learning algorithms is rather diverting towards more complexity than simplicity \cite{arras2017relevant}. While more complex models in general show higher accuracy on complex tasks \cite{richardson2018survey}, the interpretability of the systems decreases with increasing model complexity \cite{chen2018learning}. To overcome opacity of inherently non-interpretable models, they can be explained by \textit{add-on or post-hoc systems}. Add-on systems are machine learning systems that learn to generate human-readable explanations. Other explanation methods approximate elements of the system on a lower complexity scale, e.g. features with a reduced set of features, or deliver reference cases to put a classification result in perspective of similar or dissimilar cases. As a threshold for minimum explainability, \cite{goodman16eu} suggests including at least an explanation showing how input features relate to a prediction.\medskip \newline
However, with increasing opacity comes the risk of untruthfulness: \cite{lipton2016mythos} warns that plausible explanations are not necessarily truthful to the actual mechanisms and structures of the model in question. The more complex a model is, the more it needs to be reduced and simplified to match the attention span and cognitive abilities of humans \cite{kulesza2013too}. Furthermore, system designers could build untruthful yet persuasive explanations on purpose to stimulate trust building. To come to an informed judgement about a system's integrity or fairness, correct (i.e. truthful) understanding of the system is needed. Badly designed explanations likewise lead to false reassurance \cite{burrell2016machine}, a problem especially for safety-critical and high-risk domains. \cite{gilpin2018explaining} describes the challenge of generating explanations that are complete and at the same time truthful as the main challenge of the field of \textit{explainable artificial intelligence (xAI)}.\newline
An experiment of interpersonal communication by \cite{langer1978mindlessness} shows that humans tend to comply with a request in an automatic way if any explanation for the request is given. The informational content of the explanation does not play a role for the compliance rate - participants were equally likely to comply with a request given an informative (truthful) explanation as they were when given a nonsense explanation without informational content. If the same behaviour of mindlessness can be observed in interaction with decision systems that offer explanations, the risk of inappropriate trust in systems is bigger than previously assumed.\medskip \newline
We therefore investigated how different explanations (varying the level of informational content) influence the user's trust into an automatic decision system. Using the scenario of a ``social media administrator" with the task to detect offensive language in Tweets, we developed three machine learning classifiers able to process textual input and classify the texts into ``offensive" and ``not offensive" classes at varying accuracy levels. Furthermore, we implemented and validated the automatic generation of explanations at high fidelity and low fidelity levels. We measured the trust and perceived understanding in a user study with 327 participants in order to compare different classifier-explanation combinations. Our research was driven by the following research questions:\medskip \newline
\textbf{RQ 1:} What influence does the accuracy of an automatic decision system have on user's trust?\medskip \newline
\textbf{RQ 2:} Do automatically generated explanations influence user trust?\newline
\textbf{- RQ 2.1:} To what extent is user's trust influenced by the presence of explanations?\newline
\textbf{- RQ 2.2:} How does the level of truthfulness of explanations influence user trust?\medskip \newline
\textbf{RQ 3:} What role does the truthfulness of an explanation play for the user's perceived understanding?\medskip \newline




{\color{blue}
Still missing:
\begin{itemize}
	\item paragraph with key findings
	\item short reflection of contribution to the xAI research community 
\end{itemize}}

This thesis covers theory and related research projects of explainable AI in chapter 2. We give an overview over the regulations supporting transparent machine learning applications and investigate explanations in the context of human-human communication as well as human-machine communication. As trust is central to augmented intelligence, a section is devoted to trust factors and trust evaluation in the field of xAI. Chapter 3 shows the methodology of the research, including a description of a use case scenario and the evaluation setup. The implementation of three machine learning classifiers, the data processing, and the generation and validation of explanations are presented in chapter 4. We then describe the setup and results of the user study in which the influence of accuracy and explanations on user trust and perceived understanding is examined. A detailed discussion of the results is given in the last section of that chapter. Finally, an overall conclusion is drawn in chapter 6. 


