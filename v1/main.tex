\documentclass[]{article}

%packages
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{float}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage[format=plain, font=it]{caption}
\usepackage{array}
\newcommand{\head}[2]{\multicolumn{1}{>{\centering\arraybackslash}p{#1}}{\textbf{#2}}}
\usepackage{hyperref}
\usepackage{amsmath}

\begin{document}

\include{titlepage}

\thispagestyle{empty}
\begin{abstract}
	Machine learning systems have become popular in fields such as marketing, recommender systems, financing, or data mining. While they show good performance in terms of ability to correctly classify data points, complex machine learning systems pose challenges for engineers and users. Their inherent complexity makes it impossible to easily understand their structure and behaviour in order to judge on their robustness, fairness, and the correctness of statistically learned relations between variables and classes. \textit{Explainable AI (xAI)} aims to solve these challenges by modelling explanations alongside with the classifiers. By increasing the transparency of a system, engineers and users are empowered to understand and subsequently judge the classifier's behaviour. With the General Data Protection Guideline (GDPR), companies are obligated to ensure fairness in automatic profiling or automated decision making. Discovering automated discrimination in algorithms can be done by investigating the system via explanations. Other positive effects of explainability are user trust and acceptance. Inappropriate trust, however, can have harmful consequences. In safety-critical domains such as terrorism detection or physical human-robot interaction, users should not be fooled by persuasive, yet untruthful explanations. We therefore conduct a user study in which we investigate the effects of truthfulness and algorithmic performance on user trust. Our findings show that the accuracy of a classifier is more important than its transparency for user trust. Adding an explanation for a classification result can potentially harm user trust, for example when adding nonsensical (untruthful) explanations for a classifier with good or moderate accuracy. We also find that users cannot be tricked into having trust for a bad classifier with good explanations. In this research, we also compare self-reported trust to trust measured implicitly via the user's willingness to follow a classifier's prediction. The results show conflicting evidence: While users report to have highest trust in a system with high accuracy but without explanations, they show higher willingness to accept a classifier's prediction with high accuracy and meaningful explanations.
\end{abstract}

\newpage
\thispagestyle{empty}
\tableofcontents
\setcounter{tocdepth}{1}
\newpage
\thispagestyle{empty}
\listoffigures
\newpage
\thispagestyle{empty}
\listoftables
\newpage
\setcounter{page}{1}



\pagestyle{fancy}

\include{section_Acknowledgements}
\include{section_Introduction}
\include{section_Background}
\include{section_Method}
\include{section_Implementation}
\include{section_Evaluation}
\include{section_Discussion}
\include{section_Conclusion}



\bibliographystyle{abbrv}
\bibliography{mybib}

\addtocontents{toc}{\protect\setcounter{tocdepth}{0}}
\appendix
\include{section_AppendixA}
\include{section_AppendixB}

\end{document}
